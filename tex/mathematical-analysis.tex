\documentclass[thmcnt=section, 12pt]{my-elegantbook}

% index page
\usepackage{imakeidx}
\makeindex[columns = 2, intoc, options= -s index_style.ist]

% Title and author
\title{Mathematical Analysis}
\author{Isaac FEI}

\cover{cover}

\begin{document}

% Print title and cover page
\maketitle

% Print table of contents
\frontmatter
\tableofcontents
\mainmatter

%------------------------------
% Main document starts from here
%------------------------------

%==============================
\chapter{Limits and Continuity}

%------------------------------
\section{Limit of a Function}

%------------------------------
\begin{proposition} \label{pro:2}
    Let $f$ and $g$ be complex-valued functions defined on a subset $A$ of a metric space $(X, d)$, and $p$ a limit point of $A$. If $\lim_{x \to p} f(x) = 0$ and $g$ is bounded, then 
    \begin{align*}
        \lim_{x \to p} f(x)g(x) = 0
    \end{align*}
\end{proposition}

\begin{proof}
    % TODO
\end{proof}

%------------------------------

\section{Continuous Functions}

%------------------------------

The next theorem states that the composite function of continuous functions is also continuous.

\begin{theorem} \label{thm:3}
    Let $(X, d_X)$, $(Y, d_Y)$ and $(Z, d_Z)$ be metric spaces. Let $f: X \to Y$ and $g: Y \to Z$ be functions, and let $h = g \circ f$ be the composite function defined on $X$. If $f$ is continuous at $p \in X$, and $g$ is continuous at $f(p)$, then $h$ is continuous at $p$.
\end{theorem}

\begin{proof}
    Given $\varepsilon > 0$, since $g$ is continuous at $f(p)$, there exists some $\delta^\prime > 0$ such that
    \begin{align}
        d_Y(y, f(p)) < \delta^\prime
        \implies d_Z(g(y), g(f(p))) < \varepsilon
        \label{eq:7}
    \end{align}
    And since $f$ is continuous at $p$, there exists some $\delta > 0$ such that 
    \begin{align}
        d_X(x, p) < \delta
        \implies d_Y(f(x), f(p)) < \delta^\prime
        \label{eq:8}
    \end{align}
    Now, choosing $x \in X$ satisfying $d_X(x, p) < \delta$, and then applying \eqref{eq:8} followed by \eqref{eq:7}, we have
    \begin{align*}
        d_Z(g(f(x)), g(f(p))) < \varepsilon
    \end{align*} 
    This is precisely $d_Z(h(x), h(p)) < \varepsilon$. Hence, $h$ is continuous at $p$.
\end{proof}

%------------------------------

\begin{theorem} \label{thm:6}
    Let $f: X \to \R$ be a real-valued continuous function from a metric space $X$ to $\R$. Let $K \subset X$ be a compact subset. Then there exist points $p, q \in K$ such that 
    \begin{align*}
        f(p) = \sup f(K)
        \quad \text{and} \quad
        f(q) = \inf f(K)
    \end{align*}
\end{theorem}

%------------------------------

\section{Connectedness}

\begin{theorem}[Intermediate Value Theorem] \label{thm:11}
    Let $f: [a, b] \to \R$ be a real-valued continuous function. Suppose that $f(a) \neq f(b)$, and $y$ is some number in between $f(a)$ and $f(b)$. Then there exists an interior point $c \in (a, b)$ such that $f(c) = y$.
\end{theorem}

\begin{proof}
    % TODO
\end{proof}

%------------------------------

\section{Monotonic Functions}

\begin{theorem} \label{thm:15}
    If $f$ is increasing on $[a, b]$, then $f(c+)$ and $f(c-)$ both exists for every $c \in (a, b)$, and we have
    \begin{align*}
        f(c-) \leq f(c) \leq f(c+) 
    \end{align*}
    As the endpoints, we have $f(a+)$ and $f(b-)$ both exists and 
    \begin{align}
        f(a) \leq f(a+)
        \quad \text{and} \quad 
        f(b-) \leq f(b)
        \label{eq:18}
    \end{align}
    Analogous results exist for $f$ being a decreasing function.
\end{theorem}

\begin{proof}
    We only prove this theorem for increasing functions. Let $c \in (a, b)$ be an interior point. 
    
    \par (Existence of $f(c-)$) Define a set 
    \begin{align*}
        A = \set{f(x)}{a \leq x < c}
    \end{align*}
    Note that $A$ is bounded above by $f(c)$ since $f$ is increasing. Then $A$ has the least upper bound, say $\alpha = \sup A$. We want to show that $f(c-)$ exists and equals $\alpha$. Given $\varepsilon > 0$, by the property of $\alpha$, there exists $x_\varepsilon = (c - \delta) \in \left[ a, c \right) $ such that ($\delta$ depends on $\varepsilon$)
    \begin{align*}
        f(c - \delta) > \alpha - \varepsilon
    \end{align*}
    Because $f$ is increasing, we have 
    \begin{align*}
        \alpha \geq f(x) \geq f(c-\delta) > \alpha-\varepsilon
        \quad \forall x \in (c-\delta, c)
    \end{align*}
    This implies that 
    \begin{align*}
        \abs{f(x) - \alpha} < \varepsilon
        \quad \forall x \in (c-\delta, c)
    \end{align*}
    Hence, $\alpha$ is the left-hand limit of $f$ at $x = c$, i.e., $f(c-) = \alpha$. Recall $A$ is bounded above by $f(c)$. We have $\alpha \leq f(c)$, i.e., 
    \begin{align*}
        f(c-) \leq f(c)
    \end{align*}

    \par (Existence of $f(c+)$) Define 
    \begin{align*}
        B = \set{f(x)}{c < x \leq b}
    \end{align*}
    Let $\beta$ be the greatest lower bound of $B$. A similar argument shows that $f(c+)$, and $f(c+) = \beta$ with 
    \begin{align*}
        f(c) \leq f(c+)
    \end{align*}

    \par (Existence of $f(a+)$ and $f(b-)$) The existence of $f(a+)$ and $f(b-)$ along with \eqref{eq:18} can be proved by the exact argument as before.
\end{proof}

%==============================

\chapter{Differentiation}

Let our exploration begins with real-valued functions defined on real numbers.

%------------------------------

\section{Definition of Derivative}

Observe the slopes of secant lines (displayed as dashed lines) of the curve shown in Figure~\ref{fig:1}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.2]{figures/secant-lines-and-a-tangent-line.png}
    \caption{Secant lines and a tangent line of curve $y=e^x$.}
    \label{fig:1}
\end{figure}

\noindent All secant lines have a common point of intersection, $(0, 1)$. And as the other intersection point gets closer and closer to $(0, 1)$, the slope of the secant line tends to converge to some number. This number, the limiting slope, is precisely the \textbf{derivative} of this curve/function at $x=0$, which we shall soon define. We can draw a straight line passing through the point $(0,1)$ with the limiting slope. This line is then called the tangent of the curve at point $(0,1)$, as illustrated in Figure~\ref{fig:1}. 

\begin{definition}
    Let $f$ be defined on an open interval $(a, b)$, and let $c \in (a, b)$ an interior point. Then $f$ is said to be \textbf{differentiable}\index{differentiable functions} at $c$ if the limit 
    \begin{align*}
        \lim_{x \to c} \frac{f(x) - f(c)}{x - c}        
    \end{align*}
    exists. This limit, denoted by $f^\prime(c)$, is called the \textbf{derivative} \index{derivative} of $f$ at point $c$.
\end{definition}

If $f^\prime(c)$ exists $\forall c \in I$ for some interval $I$, then we can define a function $f^\prime$ on $I$, which is called the derivative of $f$. Here the word \textit{derivative} means a function instead of just a number. 

%------------------------------

The definition of derivatives we present here involves the limit obtained by letting one point approach the other one, $x \to c$. For computational convenience, we can treat the derivative of $f$ at $c$ as the limit of the fraction $[ f(c+h) - f(c) ] / h$ as $h \to 0$. That is, the limit is reached when the distance between points $x$ and $c$, $h = \abs{x-c}$, is small. The following proposition states this idea formally.

\begin{proposition} \label{pro:1}
    Let $f$ be defined on $(a, b)$. Then $f$ is differentiable at $c \in (a, b)$ if and only if 
    \begin{align}
        \lim_{h \to 0} \frac{f(c+h) - f(c)}{h} 
        \label{eq:3}
    \end{align}
    exists. In that case, $f^\prime(c) = \lim_{h \to 0} [ f(c+h) - f(c) ] / h$.
\end{proposition}

\begin{proof}
    Suppose $f$ is differentiable at $c$. For an arbitrary $\varepsilon > 0$, there exists $\delta > 0$ such that 
    \begin{align*}
        \abs{x - c} < \delta
        \implies \abs{\frac{f(x) - f(c)}{x - c} - f^\prime(c)} < \varepsilon
    \end{align*}
    Let number $h$ be such that $\abs{h} < \delta$. Then since $\abs{(c+h) - c} = \abs{h} < \delta$, we have 
    \begin{align*}
        \abs{\frac{f(c+h) - f(h)}{h} - f^\prime(c)}
        = \abs{\frac{f(c+h) - f(h)}{(c+h) - c} - f^\prime(c)} < \varepsilon
    \end{align*}
    This implies the limit \eqref{eq:3} exists, and it equals $f^\prime(c)$.

    Conversely, suppose the limit \eqref{eq:3} exists, say it equals $L$. Then for $\varepsilon > 0$ there exists $\delta > 0$ such that 
    \begin{align*}
        \abs{h} < \delta
        \implies \abs{\frac{f(c+h) - f(c)}{h} - L} < \varepsilon
    \end{align*}
    Choose $x$ such that $\abs{x - c} < \delta$, we have 
    \begin{align*}
        \abs{
            \frac{f(x) - f(c)}{x - c} - L
        } = \abs{
            \frac{f(c + (x - c)) - f(c)}{x - c} - L
        } < \varepsilon
    \end{align*}
    Therefore, the limit $[f(x) - f(c)]/(x - c)$ exists, which is equal to $L$. By the definition of derivatives, $f$ is differentiable at $c$, and $f^\prime(c) = L$.
\end{proof}

\begin{exercise}
    Calculate the derivative of $e^x$ at $x = 0$. 

    \noindent [Hint: You may use the fact $e^x = \sum_{n=0}^\infty \frac{x^n}{n!} =  1 + x + \frac{1}{2} x^2 + \frac{1}{6} x^3 + \cdots$.]
\end{exercise}

\begin{exercise}
    Calculate the derivative of
    \begin{align*}
        f(x) 
        = x^2 \sin \frac{1}{x^2} \ind\{x \neq 0\}
        = \begin{cases}
            x^2 \sin \frac{1}{x^2} &x \neq 0 \\ 
            0 &x=0
        \end{cases}
    \end{align*}
    at $x = 0$.
    \label{ex:1}
\end{exercise}

\begin{solution}
    We have 
    \begin{align*}
        \frac{f(0+h) - f(0)}{h}
        = h \sin \frac{1}{h^2}
    \end{align*}
    Note the limit of the above expression is zero as $h \to 0$ by Proposition~\ref{pro:2} since $\lim_{h\to 0} h = 0$ and $\sin (1/h^2)$ is bounded by $1$. Hence, $f^\prime(0) = 0$ by Proposition~\ref{pro:1}.

    The graph of $f(x)$ is shown in Figure~\ref{fig:2}. As we can see, $f(x)$ is clamped between curves $y=x^2$ and $y=-x^2$ with the same tangents $y=0$ at $x=0$. And we have shown $f^\prime(0)=0$, which means that the tangent of $f(x)$ is also $y=0$ at $x=0$. What conjecture can you make?

    \begin{figure}[ht]
        \centering
        \includegraphics[scale=0.2]{figures/graph-001.png}
        \caption{Graph of $f(x) = x^2 \sin \frac{1}{x^2} \ind\{x \neq 0\}$.}
        \label{fig:2}
    \end{figure}

    Moreover, we observe that $f(x)$ crosses its tangent line at $x=0$ infinitely many times. This example shows that the tangent line does not have to touch the curve only at one point.
\end{solution}

%------------------------------

\subsection{One-Sided Derivatives and Infinite Derivatives}

So far, the point at which the derivative is defined has to be an \textit{interior} point. Sometimes, we are required to consider the derivatives at the endpoints of the interval. For example, as we shall see in more detail in Chapter~\ref{chap:1}, we need to take the derivative of function $F(x) = \int_a^x f(t) \mathrm{d}t$ at $x=a$. Hence, we are motivated to define the \textbf{one-sided derivatives}\index{one-sided derivatives}. 

If we consider the derivative of $f$, $f^\prime$ as a function, then $f^\prime(x)$ may tend to infinity as $x$ approaches an endpoint. In addition, sometimes we need to interpret the meaning of vertical tangent lines. This leads to the definition of \textbf{infinite derivatives}\index{infinite derivative}.

\begin{definition}
    Let $f$ be defined on a closed interval $I$. Suppose $f$ is continuous at point $c \in I$. Then $f$ is said to have a \textbf{right-hand derivative}\index{right-hand derivative} at $c$ if the right-hand limit 
    \begin{align*}
        \lim_{x \to c^{+}} \frac{f(x) - f(c)}{x - c}
    \end{align*}
    exists as a finite number, or the limit is $\infty$ or $-\infty$. This right-hand derivative shall be denoted by $f^\prime_{+}(c)$. The \textbf{left-hand derivative}\index{left-hand derivative}, denoted by $f^\prime_{-}(c)$, is defined similarly. In addition, if $c$ is an interior point, and $f^\prime_{+}(c) = f^\prime_{-}(c) = \infty$, then we write $f^\prime(c) = \infty$. $f^\prime(c) = -\infty$ is similarly defined.
\end{definition}

\begin{remark}
    Note that though we write $f^\prime(c) = \infty$ or $f^\prime(c) = -\infty$, we do not say $f$ is differentiable there.
\end{remark}

It is not very intuitive to imagine, at first thought, a continuous function having infinite derivatives. But there are quite a lot of such examples.

\begin{example}
    The first example is constructed by cutting a circle in the middle. If we cut a circle in half and stick the lower half to the right of the upper one, then we have a function with a vertical tangent line in the middle. The explicit expression of this function is 
    \begin{align*}
        f(x) = \begin{cases}
            \sqrt{1 - (x-1)^2} &0 \leq x \leq 2 \\ 
            - \sqrt{1 - (x-3)^2} &2 < x \leq 4
        \end{cases}
    \end{align*}

    \begin{figure}[ht]
        \centering
        \includegraphics[scale=0.2]{figures/graph-002.png}
        \caption{Graph of $f(x) = \sqrt{1-(x-1)^2}\ind\{0 \leq x \leq 2\} - \sqrt{1-(x-3)^2}\ind\{2 < x \leq 4\}$.}
    \end{figure}

    \noindent The function is continuous and one can show that $f^\prime(2) = -\infty$.
    \label{eg:1}
\end{example}

\begin{example}
    The following next example includes both positive and negative intuitive derivatives, and a point at which the right and left-hand side derivatives are $\infty$ and $-\infty$, respectively. Consider the function 
    \begin{align*}
        f(x) = \sqrt[3]{x^2 (x-1) (x-2)}
    \end{align*}
    Its graph is shown in Figure~\ref{fig:3}.

    \begin{figure}[ht]
        \centering
        \includegraphics[scale=0.2]{figures/graph-003.png}
        \caption{Graph of $f(x) = \sqrt[3]{x^2 (x-1) (x-2)}$.}
        \label{fig:3}
    \end{figure}    

    It is an exercise to show $f^\prime(1) = -\infty$ and $f^\prime(2) = \infty$. We now consider the one-sided derivatives at $x = 0$. We have 
    \begin{align*}
        \frac{f(x) - f(0)}{x - 0}
        = \sqrt[3]{\frac{(x-1)(x-2)}{x}}
    \end{align*}
    Letting $x \to 0^{+}$ leads to $f^\prime_{+}(0) = \infty$, while $x \to 0^{-}$ yields $f^\prime_{-}(0) = -\infty$. Hence, we say the derivative of $f$ does not exist at $x = 0$.
    \label{eg:2}
\end{example}

%------------------------------

\section{Derivatives and Continuity}

The next theorem helps to reduce some theorems on derivatives to theorems on continuity.

\begin{theorem} \label{thm:1}
    Let $f$ be a function defined on $(a, b)$, and $c \in (a, b)$ a fixed point in that interval. We have the following statements:
    \begin{enumerate}
        \item If $f$ is differentiable at $c$, then there exists a function $\phi$ on $(a, b)$ which is continuous at $c$, and satisfies 
        \begin{align}
            f(x) - f(c) = (x - c) \phi(x)
            \quad \forall x \in (a, b)
            \label{eq:1}
        \end{align}
        And $f^\prime(c) = \phi(c)$.
        \item Conversely, if there exists a function $\phi$ on $(a, b)$ which is continuous at $c$, and satisfies \eqref{eq:1}, then $f$ is differentiable at $c$ with $f^\prime(c) = \phi(c)$.
    \end{enumerate}
\end{theorem}

The function $\phi$ is precisely the slope of function $f$'s secant line.

\begin{proof}
    (Proof of 1) Suppose $f$ is differentiable at $c$. Define 
    \begin{align*}
        \phi(x) = \frac{f(x) - f(c)}{x - c}, 
        \quad x \neq c
        \quad\text{and}\quad 
        \phi(c) = \lim_{x \to c} \frac{f(x) - f(c)}{x - c}
    \end{align*}
    Note that $\phi(c)$ is well-defined since the limit (which equals $f^\prime(c)$) indeed exists by the definition of the derivative of $f$ at $c$. Notice also that by defining function $\phi$ in this way, it is automatically continuous at $c$. And of course, the equation \eqref{eq:1} holds.

    (Proof of 2) It follows from \eqref{eq:1} that 
    \begin{align*}
        \phi(x) = \frac{f(x) - f(c)}{x - c}
        \quad \forall x \neq c
    \end{align*}
    Since $\phi$ is continuous at $c$, by sending $x \to c$, we have 
    \begin{align*}
        \phi(c) = \lim_{x \to c} \phi(x)
        = \lim_{x \to c} \frac{f(x) - f(c)}{x - c}
    \end{align*}
    This implies $f$ is differentiable at $c$, and $f^\prime(c) = \phi(c)$.
\end{proof}

%------------------------------

With the help of Theorem~\ref{thm:1}, we can easily show that $f$ must be continuous at $c$ if it is differentiable there.

\begin{theorem} \label{thm:2}
    Let $f$ be defined on $(a, b)$. If $f$ is differentiable at $c \in (a, b)$, then it is continuous at $c$.
\end{theorem}

\begin{proof}
    By Theorem~\ref{thm:1}, there exists a function $\phi$ on $(a, b)$ such that $\phi$ is continuous at $c$, and
    \begin{align}
        f(x) - f(c) = (x - c) \phi(x)
        \quad \forall x \in (a, b)
        \label{eq:2}
    \end{align}
    Since the limit of the right-hand side in \eqref{eq:2} exists as $x \to c$, the limit of the left-hand side also exists, and 
    \begin{align*}
        \lim_{x \to c} f(x) - f(c)
        = \lim_{x \to c}(x - c) \phi(x)
        = 0
    \end{align*}
    This implies 
    \begin{align*}
        \lim_{x \to c} f(x) = f(c)
    \end{align*}
    Hence, $f$ is continuous at $c$.
\end{proof}

%------------------------------

\section{Computing Derivatives}

It is difficult in practice to compute the derivative of any function only using the definition. In this section, we shall introduce some theorems and standard rules that are helpful with computation. If one knows the derivatives of elementary functions (e.g., $x^a$, $e^x$, $\ln x$, $\sin x$, $\cos x$, etc.), then by applying these theorems, one should be able to compute the derivatives of any functions that are built up out of elementary functions using summations, differences, products, quotients, and compositions.

\begin{note}
    In fact, these so-called elementary functions are not elementary at all. We shall present their definitions as well as their derivatives rigorously in later chapters.
\end{note}

%------------------------------

\subsection{Algebra of Derivatives}

%------------------------------

\begin{theorem}
    Suppose $f$ and $g$ are defined on $(a, b)$ and are both differentiable at $c \in (a, b)$. Then the functions $f + g$, $f - g$, $fg$ are also differentiable at $c$. If $g(c) \neq 0$, then the quotient $f / g$ is also differentiable at $c$. And the formulas of their derivatives are given by 
    \begin{enumerate}
        \item $(f \pm g)^\prime(c) = f^\prime(c) \pm g^\prime(c)$, 
        \item $(f g)^\prime(c) = f^\prime(c) g(c) + f(c) g^\prime(c)$, 
        \item $(f / g)^\prime(c) = \frac{f^\prime(c) g(c) - f(c) g^\prime(c)}{ g^2(c) }$, provided $g(c) \neq 0$.
    \end{enumerate}
\end{theorem}

\begin{remark}
    One might be worried about the possibility that the quotient $f / g$ may only be defined at point $c$ if we only require $g(c) \neq 0$. Then how can we say $f / g$ is differentiable at some point? However, the condition that $g$ is differentiable at $c$ implicitly implies that $g(x) \neq 0$ for $x$ in some interval about $c$ if we assume $g(c) \neq 0$. Take a few seconds and think about why this is true. Note that $g$ is continuous at $c$ by Theorem~\ref{thm:2}. Then, if $g(c) \neq 0$, by the continuity, $g$ is also nonzero in some neighborhood of $c$. Hence, in this case, $f / g$ is actually well-defined in an interval instead of at just one point.
\end{remark}

The proof we present here is done by exploiting Theorem~\ref{thm:1}, and it may differ from a traditional proof that is solely based on the definition.

\begin{proof}
    Since $f$ and $g$ are differentiable at $c$. It follows from Theorem~\ref{thm:1} that there exists functions $\phi_1$ and $\phi_2$ on $(a, b)$ that are continuous at $c$, and satisfy
    \begin{align}
        f(x) - f(c) &= (x - c) \phi_1^\prime(x) \label{eq:4}  \\
        g(x) - g(c) &= (x - c) \phi_2^\prime(x) \label{eq:5}  
    \end{align}
    with
    \begin{align}
        \phi_1 (c) = f^\prime(c)
        \quad \text{and} \quad 
        \phi_2 (c) = g^\prime(c)
        \label{eq:6}
    \end{align}

    (Proof of 1) Applying \eqref{eq:4} and \eqref{eq:5} to $(f \pm g)(x) - (f \pm g)(c)$, we have 
    \begin{align*}
        (f \pm g)(x) - (f \pm g)(c)
        = (x - c) (
            \phi_1(x) \pm \phi_2(x)
        )
    \end{align*}
    Since $\phi_1(x) \pm \phi_2(x)$ is continuous at $c$, $(f \pm g)$ is differentiable at $c$ by Theorem~\ref{thm:1}. Then by applying \eqref{eq:6}, we obtain
    \begin{align*}
        (f \pm g)^\prime (c)
        = \phi_1(c) \pm \phi_2(c)
        = f^\prime(c) \pm g^\prime(c)
    \end{align*}

    (Proof of 2) Again by applying \eqref{eq:4} and \eqref{eq:5}, we have 
    \begin{align*}
        (f g)(x) - (f g)(c)
        = (x - c) [
            \phi_1(x) g(c) 
            + f(c) \phi_2(x)
            + (x - c) \phi_1(x) \phi_2(x)
        ]
    \end{align*}
    after a few steps of algebraic calculation. It then follows from Theorem~\ref{thm:1} that $f g$ is differentiable at $c$ since the function 
    \begin{align*}
        \phi_1(x) g(c) 
            + f(c) \phi_2(x)
            + (x - c) \phi_1(x) \phi_2(x)
    \end{align*}
    is continuous at $c$. And 
    \begin{align*}
        (f g)^\prime (c)
        = \phi_1(c) g(c) 
        + f(c) \phi_2(c)
        + (c - c) \phi_1(c) \phi_2(c)
        = f^\prime(c) g(c) + f(c) g^\prime(c)
    \end{align*}

    (Proof of 3) Note that the quotient $(f / g)$ can be regarded as a product of $f$ and $1 / g$. Hence, we are going to show $1/g$ is differentiable at $c$, and then apply statement 2, which we have just proved, to conclude the proof. As explained in the remark of this theorem, $1/g$ is well-defined in a neighborhood of $c$. It follows from \eqref{eq:5} that 
    \begin{align*}
        \frac{1}{g(x)} - \frac{1}{g(c)}
        = (x - c) \frac{-\phi_2(x)}{g(x)g(c)}
    \end{align*}
    Since $-\phi_2(x) / [g(x)g(c)]$ is continuous at $c$, we conclude from Theorem~\ref{thm:1} that $1/g$ is differentiable at $c$, and 
    \begin{align*}
        \left(\frac{1}{g}\right)^\prime (c)
        = \frac{-\phi_2(c)}{g(c)g(c)}
        = \frac{-g^\prime(c)}{g^2(c)}
    \end{align*}
    Then by applying statement 2, we conclude that $f / g$ is also differentiable at $c$ with 
    \begin{align*}
        (f / g)^\prime(c) = \frac{f^\prime(c) g(c) - f(c) g^\prime(c)}{ g^2(c) }
    \end{align*}
\end{proof}

%------------------------------

\subsection{The Chain Rule}

Taking compositions is a more fundamental and natural way of combining two functions apart from summations, products, etc. The next result, the \textbf{chain rule}\index{chain rule}, provides a method of computing the derivative of a composite function.

\begin{theorem} \label{thm:4}
    Let $f$ be a function defined on an open interval $I$, and $g$ a function defined on $f(I)$. We consider the composite function $g \circ f$. If $f$ is differentiable at $c \in I$, $f(c)$ is an interior of $f(I)$, and $g$ is differentiable at $f(c)$, then $g \circ f$ is differentiable at $c$ with 
    \begin{align}
        (g \circ f)^\prime (c)
        = g^\prime(f(c)) f^\prime(c)
        \label{eq:9}
    \end{align}
\end{theorem}

With the help of Theorem~\ref{thm:1}, we can reduce the proof of this theorem to that of Theorem~\ref{thm:3}.

\begin{proof}
    Since $f$ is differentiable at $c$, using Theorem~\ref{thm:1}, we have 
    \begin{align}
        f(x) - f(c) = (x-c) \phi_1(x)
        \quad \forall x \in I
        \label{eq:10}
    \end{align}
    where $\phi_1$ is some function that is continuous at $c$ with $\phi_1(c) = f^\prime(c)$. Using the same argument, since $g$ is differentiable at $f(c)$, we have 
    \begin{align}
        g(y) - g(f(c)) = (y-f(c)) \phi_2(y)
        \quad \forall y \in f(I)
        \label{eq:11}
    \end{align}
    where $\phi_2$ is some function that is continuous at $f(c)$ with $\phi_2(c) = g^\prime(f(c))$. Replace $y$ with $f(x)$ in \eqref{eq:11} yields 
    \begin{align*}
        g(f(x)) - g(f(c)) = (f(x)-f(c)) \phi_2(f(x))
        \quad \forall x \in I
    \end{align*}
    Then by replacing $f(x)-f(c)$ with the right-hand side of \eqref{eq:10}, we have 
    \begin{align*}
        g(f(x)) - g(f(c)) = (x-c) \phi_1(x) \phi_2(f(x))
        \quad \forall x \in I
    \end{align*}
    Note that $\phi_2(f(x))$ is continuous at $c$ by Theorem~\ref{thm:3}. And the product $\phi_1(x) \phi_2(f(x))$ is also continuous. This implies $f \circ f$ is differentiable at $c$ by Theorem~\ref{thm:1}. And its derivative at $x = c$ equals
    \begin{align*}
        (g \circ f)^\prime(c)
        = \phi_1(c) \phi_2(f(c))
        = f^\prime(c) g^\prime(f(c))
    \end{align*}
    which is exactly \eqref{eq:9}.
\end{proof}

\begin{exercise}
    Calculate the derivative of
    \begin{align*}
        g(x) = x \sin \frac{1}{x} \ind\{x \neq 0\}
    \end{align*}
    for $x \neq 0$. Does the derivative exist at $x = 0$?
    \label{ex:2}
\end{exercise}

\begin{exercise}
    Calculate the derivative of 
    \begin{align*}
        f(x) = x^2 \sin \frac{1}{x^2} \ind\{x \neq 0\}
    \end{align*}
    for all $x \in \R$. Recall we have already calculated its derivative at $x = 0$ in Exercise~\ref{ex:1}, which is $f^\prime(0) = 0$.

    \noindent [Hint: Make use of the result in Exercise~\ref{ex:2} and the chain rule (Theorem~\ref{thm:4}).]
    \label{ex:3}
\end{exercise}

%------------------------------

\section{The Mean Value Theorem} 

%------------------------------

\subsection{Local Extrema}

\par One may be familiar with the fact that if the derivative of function $f$ is positive at some point, i.e., $f^\prime(c) > 0$, then $f$ is increasing near $c$, and if $f^\prime(c) < 0$, it is decreasing. However, we can extend this result a little further for we have defined one-sided and infinite derivatives.

\begin{theorem} \label{thm:14}
    Let $f$ be defined on a subset $A \subset \R$ and $c \in A$. If $f^\prime_+(c) > 0$, possibly $f^\prime_+(c) = \infty$, then there exists a half-open ball $(c, c+\delta) \subset A$ in which
    \begin{align}
        x > c \implies f(x) > f(c)
        \label{eq:12}
    \end{align}
    Similarly, if $f^\prime_{-}(c) > 0$, then $\exists (c-\delta, c) \subset A$ in which
    \begin{align*}
        x < c \implies f(x) < f(c)
    \end{align*}
    Analogous results hold if we instead assume $f^\prime_{+}(c) < 0$ or $f^\prime_{-}(c) < 0$.
\end{theorem}

\begin{proof}
    We only prove \eqref{eq:12} since the proofs of the rest of the results are similar. First suppose $f^\prime_{+}(c) < \infty$. Because the right-hand limit exists, and it equals $f^\prime_{+}(c)$, there exists $\delta > 0$ such that
    \begin{align*}
        \abs{
            \frac{f(x) - f(c)}{x - c}
            - f^\prime_{+}(c)
        } < \frac{f^\prime_{+}(c)}{2}
        \quad \forall x \in (c, c+\delta)
    \end{align*}
    This implies
    \begin{align*}
        \frac{f(x) - f(c)}{x - c} > \frac{f^\prime_{+}(c)}{2} > 0
        \quad \forall x \in (c, c+\delta)
    \end{align*}
    Then $f(x) > f(c)$ provided $x > c$ since they have the same sign.
    
    We also need to consider $f^\prime_{+}(c) = \infty$. Because the limit is infinity, there exists $\delta > 0$ such that the following fraction exceeds any predetermined number, in particular,
    \begin{align*}
        \frac{f(x) - f(c)}{x - c} > 0
        \quad \forall x \in (c, c+\delta)
    \end{align*}
    One can instead choose any large number (non-negative of course, to prove this theorem). This completes the proof since both the numerator and denominator share the same sign.
\end{proof}

\par The following corollary is the version that is more familiar to the reader and is more common in practice.

\begin{corollary} \label{cor:1}
    Let $f$ be defined on $(a, b)$, and $c \in (a, b)$ an interior point. If $f^\prime(c) > 0$, possibly $f^\prime(c) = \infty$, then there exists an open ball $B_\delta(c) \subset (a, b)$ in which
    \begin{align}
        x > c \implies f(x) > f(c)
        \quad \text{and} \quad 
        x < c \implies f(x) < f(c)
        \label{eq:14}
    \end{align}
    An analogous result holds if $f^\prime(c) < 0$.
\end{corollary}

\begin{exercise}
    Prove Corollary~\ref{cor:1} assuming $f^\prime(c) < \infty$ using Theorem~\ref{thm:1}.
\end{exercise}

\begin{solution}
    It follows from Theorem~\ref{thm:1} that 
    \begin{align}
        f(x) - f(c) = (x - c) \phi(x)
        \quad \forall x \in (a, b)
        \label{eq:13}
    \end{align}
    where $\phi(x)$ is continuous at $c$ with $\phi(c) = f^\prime(c) > 0$. Since $\phi(c) > 0$ and it is continuous there, there exists some neighborhood $B_\delta(c) \subset (a, b)$ such that $\phi(x) > 0 \; \forall x \in B_\delta(c)$. And then \eqref{eq:14} follows from \eqref{eq:13}.
\end{solution}

%------------------------------

\par The \textbf{local extremum}\index{local extremum} of a function is the largest or smallest value within some \textit{open ball}.

\begin{definition}
    Let $f$ be a real-valued function defined on a subset $A$ of a metric space $(X, d)$, and suppose $p \in A$. Then $f$ is said to have a \textbf{local maximum}\index{local maximum} at $p$ if 
    \begin{align*}
        f(x) \leq f(p)
        \quad \forall x \in B_\delta(p) \cap A
    \end{align*} 
    for some open ball $B_\delta(p)$. An analogous definition exists for \textbf{local minimum}\index{local minimum}.
\end{definition}

%------------------------------

\par The reader should be very familiar with the exercise of finding a local extremum by solving the equation in which the derivative vanishes. The following Theorem is the theory behind that.

\begin{theorem} \label{thm:5}
    Let $f$ be defined on $(a, b)$, and suppose $f$ has a local extremum at an interior point $c \in (a, b)$. If $f^\prime(c)$ exists as a finite or infinite number, then $f^\prime(c) = 0$.
\end{theorem}

\begin{remark}
    Since the conclusion is $f^\prime(c) = 0$, we may as well just assume $f$ is differentiable at $c$. The reason why we suppose $f^\prime(c)$ exists is that it makes the assumption somewhat looser.
\end{remark}

\begin{proof}
    We shall prove by contradiction. Assume $f^\prime(c) \neq 0$, then either $f^\prime(c) > 0$ or $f^\prime(c) < 0$. (We do not exclude the possibilities that $f^\prime(c) = \infty$ or $f^\prime(c) = -\infty$.) If $f^\prime(c) > 0$, then follows from Corollary~\ref{cor:1} that there acts some open ball $B_\delta(c)$ in which $f(x) > f(c)$ for $x > c$ and $f(x) < f(c)$ for $x < c$. This contradicts the fact that $f(c)$ is a local extremum. Similarly, $f^\prime(c) < 0$ will also lead to a contradiction. 
\end{proof}

\par The converse of this Theorem is not true. 

\begin{example}
    Consider $f(x) = x^3$. Its derivative is $f^\prime(x) = 3x^2$. We note that $f^\prime(0) = 0$. But it is clear that $f(0) = 0$ is neither a local maximum nor a local minimum. In fact, $f(x)$ does not have any local extremum.
\end{example}

\begin{exercise}
    Recall the function 
    \begin{align*}
        f(x) = x^2 \sin \frac{1}{x^2} \ind\{x \neq 0\}
    \end{align*}
    defined in Exercise~\ref{ex:1}. Show that $f(0)$ is not a local extremum. Recall we have shown that $f^\prime(0) = 0$. This is another example that the converse of Theorem~\ref{thm:5} does not hold.
\end{exercise}

\par Moreover, it should be emphasized that we conclude in Theorem~\ref{thm:5} that $f^\prime(c) = 0$ under the condition that $f^\prime(c)$ exists. $f(c)$ may be a local extremum without having a derivative there.

\begin{example}
    The simplest example is $f(x) = \abs{x}$. Note that $f(0) = 0$ is a local minimum. But the derivative does not exist at $x = 0$ since $f^\prime_{+}(0) = 1 \neq -1 = f^\prime_{-}(0)$.
\end{example}

%------------------------------

\subsection{Rolle's Theorem and Mean Value Theorem}

\par If a function starts from point $a$ and ends at point $b$ with the same level of height as its initial position, then there must be a turning point somewhere in between. 

\begin{theorem}[Rolle] \label{thm:9}
    Let $f$ be defined on $[a, b]$. Suppose $f^\prime(x)$ exists (as finite or infinite number) for each $x \in (a, b)$, and $f$ is continuous at the endpoints $a$ and $b$. If $f(a) = f(b)$, then there exists $c \in (a, b)$ such that 
    \begin{align*}
        f^\prime(c) = 0
    \end{align*}
\end{theorem}

\begin{proof}
    If $f$ is a constant function, i.e., $f(x) = f(a) = f(b) \; \forall x \in [a, b]$, then the conclusion is trivial since $f^\prime(x) = 0 \; \forall x \in (a, b)$.

    We now suppose $f$ is non-constant. We know that $f$ is continuous on $[a, b]$ since $f^\prime$ exists throughout $(a, b)$ and $f$ is continuous at endpoints. It then follows from Theorem~\ref{thm:6} that $f$ attains its maximum and minimum on $[a, b]$. Because we assume $f$ is non-constant and $f(a) = f(b)$, at least one of the maximum and the minimum of $f$ is reached at an interior point $c \in (a, b)$. Note that $f(c)$ is a global extremum, and of course also a local extremum. Then by applying Theorem~\ref{thm:5}, we conclude $f^\prime(c) = 0$.
\end{proof}

\begin{example}
    Review Example~\ref{eg:1}. Note that $f(0) = f(4) = 0$. And the derivative vanishes at $x = 1$ and $x = 3$. 
\end{example}

%------------------------------

\par The \textbf{mean value theorem}\index{mean value theorem} is a generalization of Rolle's theorem. Roughly speaking it says $f$ has a derivative value that equals the slope of the secant line joining two endpoints of the graph of $f$. The mean value theorem itself can be treated as a special case of an even more generalized version, the \textbf{generalized mean value theorem}\index{generalized mean value theorem} (Theorem~\ref{thm:7}).

\begin{theorem}[Mean Value Theorem] \label{thm:8}
    Suppose $f$ has a derivative (finite or infinite) at each point of $(a, b)$, and suppose $f$ is continuous at endpoints $a$ and $b$. Then there exists $c \in (a, b)$ such that 
    \begin{align*}
        f(b) - f(a) = f^\prime(c) (b - a)
    \end{align*}
\end{theorem}

\begin{theorem}[Generalized Mean Value Theorem] \label{thm:7}
    Let $f$ and $g$ be two functions, each having a derivative (finite or infinite) at each point in $(a, b)$. Suppose $f$ and $g$ are both continuous at endpoints $a$ and $b$, and $f^\prime(x)$ and $g^\prime(x)$ do not assume infinite values simultaneously. Then there exists $c \in (a, b)$ such that 
    \begin{align}
        f^\prime(c) [g(b) - g(a)]
        = g^\prime(c) [f(b) - f(a)]
        \label{eq:15}
    \end{align} 
\end{theorem}

\begin{remark}
    By taking $g(x) = x$, this theorem reduces to the standard mean value theorem, Theorem~\ref{thm:8}.
\end{remark}

\par Though it is a generalized version of the mean value theorem, and hence Rolle's theorem, it can be proved easily using Rolle's theorem by constructing a function out of $f$ and $g$.

\begin{proof}
    Define 
    \begin{align*}
        \psi(x) = f(x)[g(b) - g(a)] - g(x) [f(b) - f(a)]
    \end{align*}
    It is clear that $\psi$ is continuous on $[a, b]$. Since $f^\prime(x)$ and $g^\prime(x)$ can never both be infinity, the following equation holds:
    \begin{align*}
        \psi^\prime(x)
        = f^\prime(x)[g(b) - g(a)] - g^\prime(x) [f(b) - f(a)]
        \quad \forall x \in (a, b)
    \end{align*}
    Furthermore, we note that 
    \begin{align*}
        \psi(a) = f(a)g(b) - f(b)g(a)
        = \psi(b)
    \end{align*}
    Applying Rolle's theorem (Theorem~\ref{thm:9}) to $\psi$ leads to $\psi^\prime(c) = 0$ for some $c \in (a, b)$, which is exactly \eqref{eq:15}.
\end{proof}

%------------------------------

\par Sometimes, functions $f$ and $g$ in Theorem~\ref{thm:7} may not be defined at endpoints. But all the one-sided limits exist as finite values. In this case, we still wish to apply the theorem. The following is a slight extension of Theorem~\ref{thm:7}.

\begin{theorem}
    Let $f$ and $g$ be two functions, each having a derivative (finite or infinite) at each point in $(a, b)$. Suppose that $f^\prime(x)$ and $g^\prime(x)$ do not assume infinite values simultaneously, and one-sided limits $f(a+)$, $f(b-)$, $g(a+)$ and $g(b-)$ all exist as finite values. Then there exists $c \in (a, b)$ such that 
    \begin{align}
        f^\prime(c) [g(b-) - g(a+)]
        = g^\prime(c) [f(b-) - f(a+)]
        \label{eq:16}
    \end{align} 
\end{theorem}

\par The proof is done by simply defining the function values at the endpoints.

\begin{proof}
    Define
    \begin{align*}
        \tilde{f}(x) = f(x) \quad \forall x \in (a, b),
        \quad
        \tilde{f}(a) = f(a+),
        \quad \text{and} \quad
        \tilde{f}(b) = f(b-) \\ 
        \tilde{g}(x) = g(x) \quad \forall x \in (a, b),
        \quad
        \tilde{g}(a) = g(a+),
        \quad \text{and} \quad
        \tilde{g}(b) = g(b-)
    \end{align*}
    It is evident that $\tilde{f}$ and $\tilde{g}$ are both continuous on $[a, b]$. Furthermore, the derivatives $\tilde{f}^\prime(x) = f^\prime(x)$ and $\tilde{g}^\prime(x) = g^\prime(x)$ for each $x \in (a, b)$. Applying Theorem~\ref{thm:7} to functions $\tilde{f}$ and $\tilde{g}$ leads to \eqref{eq:16}.
\end{proof}

\par The following theorem is an immediate result of the mean value theorem, which provides a sufficient condition for determining strictly increasing and decreasing functions. It also says a function is constant if its derivative vanishes everywhere.

\begin{theorem} \label{thm:10}
    Suppose $f$ has a derivative (finite or infinite) at each point in $(a, b)$, and itself is continuous at the endpoints $a$ and $b$.
    \begin{enumerate}
        \item If $f^\prime(x) > 0 \; \forall x \in (a, b)$, then $f$ is strictly increasing on $[a, b]$.
        \item If $f^\prime(x) < 0 \; \forall x \in (a, b)$, then $f$ is strictly decreasing on $[a, b]$.
        \item If $f^\prime(x) = 0 \; \forall x \in (a, b)$, then $f$ is constant on $[a, b]$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    (Proof of 1) Let $x_1, x_2 \in [a, b]$ with $x_1 < x_2$. Applying Theorem~\ref{thm:8} to $f$ on the interval $[x_1, x_2]$ leads to 
    \begin{align*}
        f(x_2) - f(x_1) = f^\prime(c) (x_2 - x_1)
    \end{align*}
    for some $c \in (x_1, x_2)$. Since $f^\prime(c) > 0$ and $x_2 - x_1 > 0$, we have $f(x_2) > f(x_1)$. Because $x_1$ and $x_2$ are arbitrarily chosen, this implies $f$ is strictly increasing on $[a, b]$.

    (Proof of 2) Similarly, for any $x_1, x_2 \in [a, b]$ with $x_1 < x_2$, we have 
    \begin{align*}
        f(x_2) - f(x_1) = f^\prime(c) (x_2 - x_1) < 0
    \end{align*}

    (Proof of 3) For any $x_1, x_2 \in [a, b]$ with $x_1 < x_2$, by applying Theorem~\ref{thm:8}, we have
    \begin{align*}
        f(x_2) - f(x_1) = f^\prime(c) (x_2 - x_1) = 0
    \end{align*}
    This implies $f$ is a constant function.
\end{proof}

\par If $f$ and $g$ have the same derivatives everywhere in $(a, b)$, then they only differ by a constant. 

\begin{corollary}
    If $f$ and $g$ are continuous on $[a, b]$, and $f^\prime(x) = g^\prime(x) \; \forall x \in (a, b)$, then $f - g$ is constant on $[a, b]$.
\end{corollary}

\begin{proof}
    Note that the difference $f-g$ is continuous on $[a, b]$ and $(f-g)^\prime(x) = 0 \; \forall x \in (a, b)$. It then follows from Theorem~\ref{thm:10} that $f-g$ is a constant.
\end{proof}

%------------------------------

\section{Intermediate Value Theorem}

\par In Exercise~\ref{ex:3}, we have shown that the derivative of function $f(x) = x^2 \sin(1/x^2) \ind\{x \neq 0\}$ is given by 
\begin{align*}
    f^\prime(x) = \begin{cases}
        2x \sin \frac{1}{x^2} 
        - \frac{2}{x} \cos \frac{1}{x^2}
        & x \neq 0 \\
        0 & x = 0
    \end{cases}
\end{align*}
We plot the original function $f$ along with its derivative $f^\prime$ side by side in Figure~\ref{fig:4}.
\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.2]{figures/graph-004.png}
    \caption{Left: original function. Right: derivative.}
    \label{fig:4}
\end{figure}

\noindent Note that the derivative $f^\prime$ is not continuous at $x = 0$ since the limit of $f^\prime(x)$ does not exist as $x \to 0$. This example shows that the derivative of a function may not be continuous, and hence theorems for continuous functions do not apply to derivatives in general. However, the \textbf{intermediate value theorem}\index{intermediate value theorem for derivatives} is an exception. The intermediate value theorem for derivatives is also known as the \textbf{Darboux's theorem}\index{Darboux's theorem}.

\begin{theorem}[Darboux] \label{thm:12}
    Suppose $f$ is defined on $[a, b]$, and it has a derivative (finite or infinite) at each point in $(a, b)$. Assume also that the one-sided derivatives $f^\prime_{+}(a)$ and $f^\prime_{-}(b)$ both exist as finite numbers, and $f^\prime_{+}(a) \neq f^\prime_{-}(b)$. If $k$ is a number in between $f^\prime_{+}(a)$ and $f^\prime_{-}(b)$, then there exists an interior point $c \in (a, b)$ such that 
    \begin{align*}
        f^\prime(c) = k
    \end{align*}
\end{theorem}

\begin{proof}
    Note that $f$ is continuous on the entire closed interval $[a, b]$.

    \par We first consider the case where $k = [f(b) - f(a)] / (b-a)$. Then by applying the mean value theorem (Theorem~\ref{thm:8}), there exists $c \in (a, b)$ such that $f^\prime(c) = [f(b) - f(a)] / (b-a) = k$.

    \par Now, we suppose $k \neq [f(b) - f(a)] / (b-a)$. We note that either $f^\prime_{+}(a) \neq [f(b) - f(a)] / (b-a)$ or $f^\prime_{-}(b) \neq [f(b) - f(a)] / (b-a)$ since $f^\prime_{+}(a) \neq f^\prime_{-}(b)$. In other words, $k$ is either in between $f^\prime_{+}(a)$ and $[f(b) - f(a)] / (b-a)$ to it is in between $f^\prime_{-}(b)$ and $[f(b) - f(a)] / (b-a)$.

    \par Without loss of generality, we may assume $k$ is in between $f^\prime_{+}(a)$ and $[f(b) - f(a)] / (b-a)$. Define a function $\phi$ on $[a, b]$ by
    \begin{align*}
        \phi(x) = \begin{cases}
            \frac{f(x) - f(a)}{x - a}
            &x \neq a \\ 
            f^\prime_{+}(a)
            &x = a
        \end{cases}
    \end{align*}
    Note that $\phi$ is continuous on $[a, b]$, and $k$ is in between $\phi(a)$ and $\phi(b)$. Hence, we may apply the intermediate values theorem for continuous functions (Theorem~\ref{thm:11}) to $\phi$, there is a number $d \in (a, b)$ such that 
    \begin{align*}
        k = \phi(d) = \frac{f(d) - f(a)}{d - a}
    \end{align*}
    Then applying the mean value theorem (Theorem~\ref{thm:8}) to the fraction on the right-hand side of the above equation, we have 
    \begin{align*}
        k = \frac{f(d) - f(a)}{d - a}
        = f^\prime(c)
    \end{align*}
    for some $c$ in between $a$ and $d$.

    \par For the case where $k$ lying in between $f^\prime_{-}(b)$ and $[f(b) - f(a)] / (b-a)$, it can be proved with a similar argument, started by defining
    \begin{align*}
        \phi(x) = \begin{cases}
            \frac{f(x) - f(b)}{x - b}
            &x \neq b \\ 
            f^\prime_{-}(b)
            &x = b
        \end{cases}
    \end{align*}
\end{proof}

\par Essentially, Darboux's theorem tells us that though the derivative may not be continuous, it cannot have any \textit{jump discontinuities}. The following is another classical example in addition to the one in Exercise~\ref{ex:1}.

\begin{example}
    Consider the function 
    \begin{align*}
        f(x) = x^2 \sin\frac{1}{x} \ind\{x \neq 0\}
        = \begin{cases}
            x^2 \sin\frac{1}{x}
            &x \neq 0 \\ 
            0 & x = 0
        \end{cases}
    \end{align*}
    Its derivative is given by 
    \begin{align*}
        f^\prime(x)
        = \begin{cases}
            2x \sin\frac{1}{x} - \cos\frac{1}{x}
            &x \neq 0 \\ 
            0 & x = 0
        \end{cases}
    \end{align*}
    Note that the derivative $f^\prime$ is not continuous at $x = 0$. The graphs of $f$ and $f^\prime$ are shown in Figure~\ref{fig:6}.
    \begin{figure}[ht]
        \centering
        \includegraphics[scale=0.2]{figures/graph-007.png}
        \caption{Left: original function. Right: derivative.}
        \label{fig:6}
    \end{figure}
\end{example}

\par Actually, there is no need to require that $f^\prime_{+}(a)$ and $f^\prime_{-}(b)$ to be finite numbers in Theorem~\ref{thm:12}. The following theorem is an extension, and its proof is somewhat neater and more interesting than that of Theorem~\ref{thm:12}.

\begin{theorem} \label{thm:13}
    Suppose $f$ is defined on $[a, b]$, and it has a derivative (finite or infinite) at each point in $(a, b)$. Assume also that the one-sided derivatives $f^\prime_{+}(a)$ and $f^\prime_{-}(b)$ both exist (each of the two can either be finite or infinite), and $f^\prime_{+}(a) \neq f^\prime_{-}(b)$. If $k$ is a number in between $f^\prime_{+}(a)$ and $f^\prime_{-}(b)$, then there exists an interior point $c \in (a, b)$ such that 
    \begin{align*}
        f^\prime(c) = k
    \end{align*}
\end{theorem}

\begin{proof}
    Without loss of generality, we may assume $f^\prime_{+}(a) < k <  f^\prime_{-}(b)$. Define a function $g$ on $[a, b]$ by the equation
    \begin{align*}
        g(x) = f(x) - kx
    \end{align*}
    Note that $g$ is continuous on $[a, b]$, and the derivative of $g$ exists at each point in $(a, b)$. Furthermore, the one-sided derivatives of $g$, $g^\prime_{+}(a)$ and $g^\prime_{-}(b)$, both exist, which are given by
    \begin{align}
        g^\prime_{+}(a) = f^\prime_{+}(a) - k
        \quad \text{and} \quad 
        g^\prime_{-}(b) = f^\prime_{-}(b) - k
        \label{eq:17}
    \end{align} 
    It should be emphasized that \eqref{eq:17} also holds for infinite values. 

    \par Because we assume $f^\prime_{+}(a) < k <  f^\prime_{-}(b)$, \eqref{eq:17} yields
    \begin{align*}
        g^\prime_{+}(a) < 0
        \quad \text{and} \quad 
        g^\prime_{-}(b) > 0
    \end{align*} 
    It then follows from Theorem~\ref{thm:14} that there exist points $t_1$ and $t_2$ at which
    \begin{align*}
        g(t_1) < g(a)
        \quad \text{and} \quad 
        g(t_2) < g(b)
    \end{align*}
    This implies that $g$ must attain its minimum at some interior point $c \in (a, b)$ rather than at the endpoints. Then by applying Theorem~\ref{thm:5}, we have 
    \begin{align*}
        g^\prime(c) = 0
    \end{align*}
    Because $g^\prime(x) = f^\prime(x) - k \; \forall x \in (a, b)$, it follows that
    \begin{align*}
        f^\prime(c) = k
    \end{align*}
    This completes the proof.
\end{proof}

\begin{example}
    Consider a function defined on $[0, 1]$ given by 
    \begin{align*}
        f(x) = \sqrt[3]{x(x-1)},
        \quad x \in [0, 1]
    \end{align*}
    Note that the derivative exists as a finite number in $(0, 1)$. And it can be shown that $f^\prime_{+}(0) = -\infty$ and $f^\prime_{-}(1) = \infty$. Then Theorem~\ref{thm:13} tells us $f^\prime$ takes all the real numbers in $(0, 1)$. See Figure~\ref{fig:5}.
    \begin{figure}[ht]
        \centering
        \includegraphics[scale=0.2]{figures/graph-006.png}
        \caption{Left: original function. Right: derivative.}
        \label{fig:5}
    \end{figure}

    \begin{note}
        Theoretically, the graph of $f^\prime$ in Figure~\ref{fig:5} will tend to $-\infty$ and $\infty$ as $x$ approaches $0$ and $1$, respectively. The reason why the absolute values of the derivatives near the endpoints shown in the graph are only about $3000$ is because of the limited computational precision of the computer.
    \end{note}
\end{example}

%------------------------------

\par By Theorem~\ref{thm:10}, we know a function is strictly increasing if its derivative is always positive, and it is strictly decreasing if its derivative is always negative. Suppose now we only know that $f^\prime$ is nonzero in some open interval, what can we conclude? Thanks to the intermediate value theorem, or Darboux's theorem, $f^\prime$ is either always positive or always negative. Hence, we can conclude that $f$ must be strictly monotonic. 

\begin{theorem}
    Suppose $f$ has a derivative (finite or infinite) in $(a, b)$, and is continuous at endpoints $a$ and $b$. If $f^\prime(x) \neq 0 \; \forall x \in (a, b)$, then $f$ is strictly monotonic on $[a, b]$.
\end{theorem}

\begin{proof}
    Pick a point $x_1 \in (a, b)$. Suppose $f^\prime(x_0) > 0$. Let $x_0$ be fixed, and choose an arbitrary point $x \in (a, b)$ other than $x_0$. We claim that $f^\prime(x) > 0$. Otherwise, if $f^\prime(x) < 0$, then by Theorem~\ref{thm:13}, there exists some point $c$ in between $x_0$ and $x$ such that $f^\prime(c) = 0$, which contradicts the given condition that $f^\prime$ is nonzero in $(a, b)$. In this case, we have $f^\prime(x) > 0 \; \forall x \in (a, b)$. It then follows from Theorem~\ref{thm:10} that $f$ is strictly increasing on $[a, b]$.

    \par If $f^\prime(x_0) < 0$, then a similar argument will show that $f$ is strictly decreasing on $[a, b]$.
\end{proof}

%------------------------------

\par It is because the derivatives cannot have jump discontinuities that monotonic derivatives are necessarily continuous.

\begin{theorem}
    Suppose $f^\prime$ exits, and is monotonic in $(a, b)$. Then $f^\prime$ is continuous in $(a, b)$.
\end{theorem}

\begin{proof}
    Without loss of generality, suppose that $f^\prime$ is increasing. We shall prove by contradiction. Assume that $f^\prime$ is discontinuous at $x = c \in (a, b)$. By Theorem~\ref{thm:15}, we have $f^\prime(c-)$ and $f^\prime(c+)$ both exist, and 
    \begin{align*}
        f^\prime(c-) \leq f(c) \leq f^\prime(c+)
    \end{align*}
    But because we assume $f^\prime$ is not continuous at $x = c$, it holds that 
    \begin{align*}
        f^\prime(c-) < f^\prime(c+)
    \end{align*}
    Let $[c-\delta, c+\delta]$ ($\delta > 0$) be a closed interval contained in $(a, b)$. We have 
    \begin{align*}
        f^\prime(c-\delta) \leq f^\prime(c-) < f^\prime(c+) \leq f^\prime(c+\delta)
    \end{align*}
    Let $k$ be a number in between $f^\prime(c-)$ and $ f^\prime(c+)$ other than $f^\prime(c)$. That is, 
    \begin{align*}
        f^\prime(c-) < k < f^\prime(c+) 
        \quad \text{and} \quad 
        k \neq f^\prime(c)
    \end{align*}
    
    \par Applying Theorem~\ref{thm:13} to $f^\prime$ on $[c-\delta, c+\delta]$, we conclude that there exists $x_0 \in (c-\delta, c+\delta)$ such that $f^\prime(x_0) = k$. But since $f^\prime$ is increasing, $f^\prime(c-) < k = f^\prime(x_0)$ and $k \neq f^\prime(c)$, we have $x_0 > c$. On the other hand, because $ f^\prime(x_0) = k < f^\prime(c+) $ and $k \neq f^\prime(c)$, we have $x_0 < c$. This leads to a contradiction. 
\end{proof}

%------------------------------

\section{Taylor's Theorem}

\par If $f$ is differentiable at $x = c$, then it can be approximated by 
\begin{align}
    f(x) \approx f(c) + f^\prime(c)(x-c)
    \label{eq:19}
\end{align}
when $\abs{x-c}$ is small. Geometrically, $f$ is close to its tangent line at $x=c$ near the point of tangency. Another way to look at \eqref{eq:19} is that $f$ is approximated by a polynomial of degree one whose first-order derivative at $x=c$ is exactly $f^\prime(c)$. And the value of this polynomial at $x=c$ is $f(c)$, that is, its zeroth-order derivative is $f(c)$. We can extend this idea by approximating $f$ with higher degree polynomials whose derivatives of all orders match with that of $f$ at $x = c$.

\begin{example}
    Suppose we know any order derivative of the function $f(x) = e^x$ is itself. That is,
    \begin{align*}
        f^{(k)}(x) = f(x) = e^x \quad \forall k = 0, 1, 2, \ldots
    \end{align*}
    Consider the polynomial 
    \begin{align*}
        p_n(x) 
        = \sum_{k=0}^n \frac{x^k}{k!}
        = 1 + x + \frac{x^2}{2} + \frac{x^3}{6} + \cdots + \frac{x^n}{n!}
    \end{align*}
    One can check that 
    \begin{align*}
        p^{(k)}_n(0) = f^{(k)}(0)
        \quad \forall k = 0, 1, \ldots, n
    \end{align*}
    Hence, it is reasonable to approximate $f(x)$ with $p_n(x)$ near point $x = 0$. Figure~\ref{fig:7} compares $f$ with $p_n$ with several choices of $n$.

    \begin{figure}[ht]
        \centering
        \includegraphics[scale=0.2]{figures/graph-008.png}
        \caption{Approximations of $f$ at $x=0$.}
        \label{fig:7}
    \end{figure}
\end{example}

%------------------------------

In general, a function $f$ can be approximated by the polynomial
\begin{align*}
    \sum_{k=0}^{n-1} \frac{f^{(k)}(c)}{k}(x-c)^k
    = f(c) + f^\prime(c)(x-c) + \cdots + \frac{f^{(n)}(c)}{n!} (x-c)^{n}
\end{align*}
at point $c$ provided that $f$ has up to $(n+1)$-th order derivative. The reason why we approximate $f$ with a polynomial of $n$ degree is that we have a remainder term, which makes use of the $(n+1)$-th order derivative. The remainder tells us how accurate the approximation is. The formal statement is described in the following theorem, the \textbf{Taylor's theorem}\index{Taylor's theorem}.

\begin{theorem}[Taylor] \label{thm:16}
    Let $f$ be a function having $(n+1)$-th order derivative everywhere in $(a, b)$, and suppose $f^{(n)}$ is continuous on $[a, b]$. Let $c \in (a, b)$ be an interior point. Then for every $x \in [a, b]$ other than $c$, we have 
    \begin{align}
        f(x) = \sum_{k=0}^{n} \frac{f^{(k)}(c)}{k!}(x-c)^k + \frac{f^{(n+1)}(\xi)}{(n+1)!} (x-c)^{n+1}
        \label{eq:20}
    \end{align}
    where $\xi$ is some number in between $x$ and $c$.
\end{theorem}

\par This theorem can be obtained as an immediate corollary of the following extension of the mean value theorem.

\begin{theorem} \label{thm:17}
    Let $f$ and $g$ be two functions each having $(n+1)$-th order derivative everywhere in $(a, b)$. Suppose that $f^{(n)}$ and $g^{(n)}$ are both continuous on $[a, b]$. Let $c \in (a, b)$ be an interior point. Then for every $x \in [a, b]$ other than $c$, we have 
    \begin{align}
        \left[
            f(x) - \sum_{k=0}^{n} \frac{f^{(k)}(c)}{k!}(x-c)^k
        \right] g^{(n+1)}(\xi)
        = f^{(n+1)}(\xi) \left[
            g(x) - \sum_{k=0}^{n} \frac{g^{(k)}(c)}{k!}(x-c)^k
        \right] 
        \label{eq:21}
    \end{align}
    where $\xi$ is some number in between $x$ and $c$.
\end{theorem}

\begin{remark}
    Taking $g(x) = (x-c)^{n+1}$, we have
    \begin{align*}
        g^{(n+1)}(\xi) = (n+1)!
        \quad \text{and} \quad
        \sum_{k=0}^{n} \frac{g^{(k)}(c)}{k!}(x-c)^k = 0
    \end{align*}
    Then \eqref{eq:21} reduces to \eqref{eq:20}, and hence this theorem reduces to Theorem~\ref{thm:16}.
\end{remark}

\begin{proof}
    Define functions $F(t)$ and $G(t)$ on $[a, b]$ by 
    \begin{align*}
        F(t) = \sum_{k=0}^{n} \frac{f^{(k)}(t)}{k!}(x-t)^k
        \quad \text{and} \quad 
        G(t) = \sum_{k=0}^{n} \frac{g^{(k)}(t)}{k!}(x-t)^k
        \quad \forall t \in [a, b]
    \end{align*}
    Note that $F$ and $G$ are continuous on $[a, b]$, and they have derivatives everywhere in $(a, b)$. Applying the generalized mean value theorem (Theorem~\ref{thm:7}) to $F(t)$ and $G(t)$ on the closed interval joining points $x$ and $c$, we have 
    \begin{align}
        [F(x) - F(c)] G^\prime(\xi)
        = F^\prime(\xi) [G(x) - G(c)]
        \label{eq:22}
    \end{align}
    for some number $\xi$ in between $x$ and $c$.

    \par After a few steps of computation, we obtain derivatives $F^\prime(t)$ and $G^\prime(t)$ as follows.
    \begin{align}
        F^\prime(t) = \frac{f^{(n+1)}(t)}{n!}(x-t)^{n}
        \quad \text{and} \quad 
        G^\prime(t) = \frac{g^{(n+1)}(t)}{n!}(x-t)^{n}
        \quad \forall t \in (a, b)
        \label{eq:23}
    \end{align}
    We also note that 
    \begin{align}
        F(x) = f(x), 
        \quad
        F(c) = \sum_{k=0}^{n} \frac{f^{(k)}(c)}{k!}(x-c)^k, 
        \quad
        G(x) = g(x), 
        \quad \text{and} \quad
        G(c) = \sum_{k=0}^{n} \frac{g^{(k)}(c)}{k!}(x-c)^k
        \label{eq:24}
    \end{align}
    Plugging \eqref{eq:23} and \eqref{eq:24} into \eqref{eq:22} leads to \eqref{eq:21}.
\end{proof}

\par We can rewrite \eqref{eq:20} as 
\begin{align*}
    f(x) = p_n(x) + r_n(x)
\end{align*}
where 
\begin{align*}
    p_n(x) = \sum_{k=0}^{n} \frac{f^{(k)}(c)}{k!}(x-c)^k
\end{align*}
is the polynomial approximation, and 
\begin{align*}
    r_n(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!} (x-c)^{n+1}
\end{align*}
is the remainder. The smaller the absolute value of $r_n(x)$ is, the more accurate the approximation will be.


\begin{exercise}
    Use Taylor's theorem (Theorem~\ref{thm:16}) to approximate the value of $\ln(0.6)$ so that the absolute error is less than $0.1$.
\end{exercise}

\begin{solution}
    Putting $f(x) = \ln(x)$ in \eqref{eq:20}, we have
    \begin{align*}
        p_n(x;c) &= \sum_{k=0}^{n} \frac{f^{(k)}(c)}{k!}(x-c)^k
        = \ln(c) + \sum_{k=1}^{n} \frac{(-1)^{k+1}}{k c^n}(x-c)^k \\ 
        r_n(x;c) &= \frac{(-1)^{n+2}}{(n+1) \xi^{n+1}}(x-c)^{n+1}
    \end{align*}
    Since the value of $\ln(x)$ at $x=1$ is known, i.e., $\ln(1) = 0$, we can expand it about that point by putting $c=1$. Then we have 
    \begin{align*}
        p_n(x) = \sum_{k=1}^{n} \frac{(-1)^{k+1}}{k}(x-1)^k
        \quad \text{and} \quad
        r_n(x) = \frac{(-1)^{n+2}}{(n+1) \xi^{n+1}}(x-1)^{n+1}
    \end{align*}
    To approximate $\ln(0.6)$ by $p_n(0.6)$ within the error bounds, we need to estimate the remainder $r_n(0.6)$. Note that $\xi$ is now some number in between $0.6$ and $1$ since $x=0.6$ and $c=1$. The absolute value of $r_n(0.6)$ is bounded above by 
    \begin{align}
        \abs{r_n(0.6)} = \frac{1}{n+1} \abs{\frac{0.6-1}{\xi}}^{n+1}
        < \frac{1}{n+1} \left(\frac{0.4}{0.6}\right)^{n+1}
        = \frac{1}{n+1} \left(\frac{2}{3}\right)^{n+1}
        \label{eq:25}
    \end{align}


    \par We then estimate $\abs{r_n(0.6)}$ using \eqref{eq:25} by trying the first few values of $n$. We have 
    \begin{align*}
        r_1(0.6) = \frac{2}{9} > 0.1
        \quad \text{and} \quad 
        r_2(0.6) = \frac{8}{81} < 0.1
    \end{align*}
    Hence, it is adequate to approximate $\ln(0.6)$ by $p_2(0.6)$. The approximated value is
    \begin{align*}
        \ln(0.6) \approx p_2(0.6) = (0.6-1) - \frac{1}{2} \times (0.6-1)^2 = -0.48
    \end{align*}
    We can also visualize the result in Figure~\ref{fig:8}. As we can see, the approximated value is quite close to the true value. And the absolute error is actually much less than the required bound, $0.1$.
    \begin{figure}[ht]
        \centering
        \includegraphics[scale=0.2]{figures/taylor-approx.png}
        \caption{Approximation of $\ln(0.6)$.}
        \label{fig:8}
    \end{figure}
\end{solution}

%==============================

\chapter{Functions of Bounded Variation}

%------------------------------

\section{Characterization of Functions of Bounded Variation}

\begin{theorem}[Characterization of Functions of Bounded Variation] \label{thm:22}
    Let $f$ be a real-valued function on $[a, b]$. Then, the following statements are equivalent.
    \begin{enumerate}
        \item $f$ is of bounded variation on $[a, b]$.
        \item There exist two increasing functions $g$ and $h$ on $[a, b]$ such that $f = g - h$.
        \item There exist two \textbf{strictly} increasing functions $g$ and $h$ on $[a, b]$ such that $f = g - h$.
    \end{enumerate}
\end{theorem}

%==============================
\chapter{Riemann-Stieltjes Integral} \label{chap:1}

\section{Definition of the Riemann-Stieltjes Integral}

%------------------------------

\begin{definition} \label{def:1}
    Suppose that $f$ and $\alpha$ are \textbf{real-valued bounded} functions on $[a,b]$. Let $P = \left\{x_0, x_1, \ldots, x_n\right\}$ be a partition on $[a, b]$ and $t_k$ a point in the sub-interval $[x_{k-1}, x_k]$. A sum of the form 
    \begin{align*}
        S(P,f,\alpha)
        = \sum_{k=1}^n f(t_k) \Delta \alpha_k
    \end{align*}
    is called a Riemann-Stieltjes sum of $f$ with respect to $\alpha$. We say $f$ is Riemann-integrable with respect to $\alpha$, and write $f \in \mathfrak{R}(\alpha)$ on $[a,b]$ if there exists a number $A$ having the following property: for any given $\varepsilon > 0$, there exists a partition $P_\varepsilon$ such that 
    \begin{align*}
        \abs{S(P,f,\alpha) - A} < \varepsilon
    \end{align*}
    for any refinement $P$ of $P_\varepsilon$ and for any choice of points $t_k$. (Note that $S(P,f,\alpha)$ depends on $t_k$.) Moreover, the number $A$ is \textbf{uniquely} determined if it exists (this is proved in Proposition~\ref{pro:3}) and is denoted by \begin{align*}
        \int_a^b f \; \mathrm{d}\alpha
    \end{align*}
\end{definition}

\begin{note}
    To ease the nation, we will sometimes write
    \begin{align*}
        \sum_{k} f(t_k) \delta \alpha_k
    \end{align*}
    without specifying the lower and upper limits of $k$ if no confusion will be caused.
\end{note}

%------------------------------

\par Recall Definition~\ref{def:1} only requires the existence of $A$. We now show that such number $A$ is also unique.

\begin{proposition} \label{pro:3}
    Let $S(P,f,\alpha)$ be as in Definition~\ref{def:1}. If $A$ and $A^\prime$ both satisfy the property stated in Definition~\ref{def:1}, then $A = A^\prime$.
\end{proposition}

\begin{proof}
    Given $\varepsilon > 0$, by the property in Definition~\ref{def:1}, there exist partitions $P_1$ and $P_2$ such that  
    \begin{align*}
        \abs{S(P_1,f,\alpha) - A} &< \varepsilon / 2 &
        \abs{S(P_2,f,\alpha) - A^\prime} &< \varepsilon / 2
    \end{align*}
    Let $P = P_1 \cup P_2$. We have
    \begin{align*}
        \abs{S(P,f,\alpha) - A} &< \varepsilon / 2 &
        \abs{S(P,f,\alpha) - A^\prime} &< \varepsilon / 2
    \end{align*}
    It then follows that
    \begin{align*}
        \abs{A - A^\prime}
        \leq \abs{S(P,f,\alpha) - A} +
        \abs{S(P,f,\alpha) - A^\prime} 
        < \varepsilon / 2 + \varepsilon / 2
        = \varepsilon
    \end{align*}
    Since $\varepsilon > 0$ is arbitrary, we must have $A = A^\prime$.
\end{proof}

%------------------------------

\section{Linear Properties}

%------------------------------

\par The integral is linear in the integrand. In other words, the integral of a linear combination of functions is equal to the linear combination of integrals of each function.

\begin{theorem} \label{thm:18}
    Suppose that $f, g \in \mathfrak{R}(\alpha)$ on $[a,b]$. Then $c_1 f + c_2 g \in \mathfrak{R}(\alpha)$ on $[a,b]$ where $c_1$ and $c_2$ are constants. In that case, 
    \begin{align*}
        \int_a^b c_1 f + c_2 g \; \mathrm{d}\alpha
        = c_1 \int_a^b f \; \mathrm{d}\alpha
        + c_2 \int_a^b g \; \mathrm{d}\alpha
    \end{align*}
\end{theorem}

\begin{proof}
    Let $P$ be a partition on $[a,b]$. The Riemann-Stieltjes sum of $c_1 f + c_2 g$ can be written as 
    \begin{align*}
        S(P, c_1 f + c_2 g, \alpha)
        &= \sum_{k} c_1 f(t_k) + c_2 g(t_k) \Delta\alpha_k \\ 
        &= c_1 \sum_{k} f(t_k) \Delta\alpha_k
        + c_2 \sum_{k} g(t_k) \Delta\alpha_k \\ 
        &= c_1 S(P, f, \alpha) + c_2 S(P, g, \alpha)
    \end{align*}
    Given $\varepsilon > 0$. Since $f \in \mathfrak{R}(\alpha)$ on $[a,b]$ then there exists a partition $P_\varepsilon^{\prime}$
    such that 
    \begin{align*}
        \abs{S(P, f, \alpha) - \int_a^b f \; \mathrm{d}\alpha} < \frac{\varepsilon / 2}{1 + \abs{c_1}}
        \quad \forall P \supset P_\varepsilon^{\prime}
    \end{align*}
    Similarly, since $g \in \mathfrak{R}(\alpha)$, there exists a partition $P_\varepsilon^{\prime\prime}$ such that 
    \begin{align*}
        \abs{S(P, g, \alpha) - \int_a^b g \; \mathrm{d}\alpha} < \frac{\varepsilon / 2}{1 + \abs{c_2}}
        \quad \forall P \supset P_\varepsilon^{\prime\prime}
    \end{align*}
    Let $P_\varepsilon$ be the refinement of $P_\varepsilon^{\prime}$ and $P_\varepsilon^{\prime\prime}$, i.e., $P = P_\varepsilon^{\prime} \cup P_\varepsilon^{\prime\prime}$. Then for any $P \supset P_\varepsilon$, we have 
    \begin{align*}
        &\abs{
            S(P, c_1 f + c_2 g, \alpha) 
            - c_1 \int_a^b f \; \mathrm{d}\alpha
            - c_2 \int_a^b g \; \mathrm{d}\alpha
        } \\
        &\leq \abs{c_1} \abs{S(P,f,\alpha) - \int_a^b f \; \mathrm{d}\alpha}
        + \abs{c_2} \abs{S(P,g,\alpha) - \int_a^b g \; \mathrm{d}\alpha} \\ 
        &< \abs{c_1} \frac{\varepsilon / 2}{1 + \abs{c_1}}
        + \abs{c_2} \frac{\varepsilon / 2}{1 + \abs{c_2}} \\ 
        &< \varepsilon
    \end{align*}
    This completes the proof.
\end{proof}

%------------------------------

\par The integral is also linear in the integrator.

\begin{theorem} \label{thm:19}
    If $f \in \mathfrak{R}(\alpha)$ and $f \in \mathfrak{R}(\beta)$ on $[a,b]$, then $f \in \mathfrak{R}(c_1 \alpha + c_2 \beta)$ where $c_1$ and $c_2$ are constants. In that case, 
    \begin{align*}
        \int_a^b f \; \mathrm{d}(c_1 \alpha + c_2 \beta)
        = c_1 \int_a^b f \; \mathrm{d}\alpha
        + c_2 \int_a^b f \; \mathrm{d}\beta
    \end{align*}
\end{theorem}

\begin{proof}
    Let $P$ be a partition on $[a,b]$. We have 
    \begin{align*}
        S(P, f, c_1 \alpha + c_2 \beta)
        &= \sum_{k=0} f(t_k) + \Delta(c_1 \alpha + c_2 \beta)_k \\ 
        &= c_1 \sum_{k=0} f(t_k) \Delta\alpha_k
        + c_2 \sum_{k=0} f(t_k) \Delta\beta_k \\ 
        &= c_1 S(P, f, \alpha) + c_2 S(P, f, \beta)
    \end{align*}
    Given $\varepsilon > 0$. Since $f \in \mathfrak{R}(\alpha)$ on $[a,b]$ then there exists a partition $P_\varepsilon^{\prime}$
    such that 
    \begin{align*}
        \abs{S(P, f, \alpha) - \int_a^b f \; \mathrm{d}\alpha} < \frac{\varepsilon / 2}{1 + \abs{c_1}}
        \quad \forall P \supset P_\varepsilon^{\prime}
    \end{align*}
    Similarly, since $f \in \mathfrak{R}(\beta)$, there exists a partition $P_\varepsilon^{\prime\prime}$ such that 
    \begin{align*}
        \abs{S(P, f, \beta) - \int_a^b f \; \mathrm{d}\beta} < \frac{\varepsilon / 2}{1 + \abs{c_2}}
        \quad \forall P \supset P_\varepsilon^{\prime\prime}
    \end{align*}
    Let $P = P_\varepsilon^{\prime} \cup P_\varepsilon^{\prime\prime}$. Then for any $P \supset P_\varepsilon$, we have 
    \begin{align*}
        &\abs{
            S(P, f, c_1 \alpha + c_2 \beta) 
            - c_1 \int_a^b f \; \mathrm{d}\alpha
            - c_2 \int_a^b f \; \mathrm{d}\beta
        } \\
        &\leq \abs{c_1} \abs{S(P,f,\alpha) - \int_a^b f \; \mathrm{d}\alpha}
        + \abs{c_2} \abs{S(P,f,\beta) - \int_a^b f \; \mathrm{d}\beta} \\ 
        &< \abs{c_1} \frac{\varepsilon / 2}{1 + \abs{c_1}}
        + \abs{c_2} \frac{\varepsilon / 2}{1 + \abs{c_2}} \\ 
        &< \varepsilon
    \end{align*}
\end{proof}

%------------------------------

\par If we divide the interval $[a, b]$ into two parts with some point $c \in (a, b)$ in the middle, then the integral over the entire interval is the sum of the integrals on these two sub-intervals. This is also a kind of linearity of integrals considering the interval of integration. 

\begin{lemma}
    Suppose $c \in (a, b)$. We have 
    \begin{align}
        \int_a^b f \; \mathrm{d}\alpha
        = \int_a^c f \; \mathrm{d}\alpha
        + \int_c^b f \; \mathrm{d}\alpha
        \label{eq:27}
    \end{align}
    The existence of two integrals in \eqref{eq:27} will imply the existence of the third one.
\end{lemma}

\begin{proof}
    We have 
    \begin{align}
        S(P,f,\alpha) = S(P^\prime, f, \alpha) + S(P^{\prime\prime}, f, \alpha)
        \quad
        \forall P = P^\prime \cup P^{\prime\prime}
        \label{eq:26}
    \end{align}
    where $P$, $P^\prime$ and $P^{\prime\prime}$ are partitions on $[a, b]$, $[a, c]$ and $[c, b]$, respectively. Let $\varepsilon > 0$ be chosen arbitrarily.

    \par (Proof of existence of $\int_a^b f \; \mathrm{d}\alpha$) Assume $\int_a^c f \; \mathrm{d}\alpha$ and $\int_c^b f \; \mathrm{d}\alpha$ exist. Then 
    \begin{align*}
        \abs{S(P^\prime, f, \alpha) - \int_a^c f \; \mathrm{d}\alpha} < \varepsilon / 2
        \quad \forall P^\prime \supset P_\varepsilon^\prime
    \end{align*}
    for some $P_\varepsilon^\prime$ on $[a, c]$. And 
    \begin{align*}
        \abs{S(P^{\prime\prime}, f, \alpha) - \int_a^c f \; \mathrm{d}\alpha} < \varepsilon / 2
        \quad \forall P^{\prime\prime} \supset P_\varepsilon^{\prime\prime}
    \end{align*}
    for some $P_\varepsilon^{\prime\prime}$ on $[c, b]$. Let
    \begin{align*}
        P_\varepsilon = P_\varepsilon^\prime \cup P_\varepsilon^{\prime\prime}
    \end{align*}
    (Note that $c \in P_\varepsilon$.) Let
    \begin{align*}
        P &\supset P_\varepsilon &
        P^\prime &= P \cap [a, c] &
        P^{\prime\prime} &= P \cap [c, b]
    \end{align*}
    Observe that
    \begin{align*}
        P^\prime &\supset P_\varepsilon^\prime & 
        P^{\prime\prime} &\supset P_\varepsilon^{\prime\prime}
    \end{align*}
    It then follows from \eqref{eq:26} that
    \begin{align*}
        &\abs{
            S(P, f, \alpha)
            - \int_a^c f \; \mathrm{d}\alpha
            - \int_c^b f \; \mathrm{d}\alpha 
        } \\ 
        &\leq \abs{
            S(P^\prime, f, \alpha)
            - \int_a^c f \; \mathrm{d}\alpha
        } + \abs{
            S(P^{\prime\prime}, f, \alpha)
            - \int_c^b f \; \mathrm{d}\alpha
        } \\ 
        &< \varepsilon / 2 + \varepsilon / 2 \\ 
        &= \varepsilon
    \end{align*} 
    Therefore, $f \in \mathfrak{R}(\alpha)$ on $[a, b]$ and \eqref{eq:27} holds.

    \par (Proof of existence of $\int_c^b f \; \mathrm{d}\alpha$) Assume $\int_a^b f \; \mathrm{d}\alpha$ and $\int_a^c f \; \mathrm{d}\alpha$ exist. Then 
    \begin{align*}
        \abs{S(P, f, \alpha) - \int_a^b f \; \mathrm{d}\alpha} < \varepsilon / 2
        \quad \forall P \supset P_\varepsilon
    \end{align*}
    for some $P_\varepsilon$ on $[a, b]$. And
    \begin{align*}
        \abs{S(P^\prime, f, \alpha) - \int_a^c f \; \mathrm{d}\alpha} < \varepsilon / 2
        \quad \forall P^\prime \supset P_\varepsilon^\prime
    \end{align*}
    for some $P_\varepsilon^\prime$ on $[a, c]$.
    Let
    \begin{align*}
        P_\varepsilon^{\prime\prime} = (P_\varepsilon \cup P_\varepsilon^\prime) \cap [c, b]
    \end{align*}
    Let
    \begin{align*}
        P^{\prime\prime} &\supset P_\varepsilon^{\prime\prime} &
        P^\prime &\supset (P_\varepsilon \cup P_\varepsilon^\prime) \cap [a, c] &
        P &= P^\prime \cup P^{\prime\prime}
    \end{align*} 
    Observe that 
    \begin{align*}
        P^\prime &\supset P_\varepsilon^\prime &
        P &= P^\prime \cup P^{\prime\prime}
        \supset (P_\varepsilon \cap [a, c]) \cup (P_\varepsilon \cap [c, b])
        = P_\varepsilon
    \end{align*}
    It then follows from \eqref{eq:26} that
    \begin{align*}
        &\abs{
            S(P^{\prime\prime}, f, \alpha)
            - \int_a^b f \; \mathrm{d}\alpha
            + \int_a^c f \; \mathrm{d}\alpha 
        } \\ 
        &\leq \abs{
            S(P, f, \alpha)
            - \int_a^b f \; \mathrm{d}\alpha
        } + \abs{
            S(P^\prime, f, \alpha)
            - \int_a^c f \; \mathrm{d}\alpha
        } \\ 
        &< \varepsilon / 2 + \varepsilon / 2 \\ 
        &= \varepsilon
    \end{align*} 
    Therefore, $f \in \mathfrak{R}(\alpha)$ on $[c, b]$ and \eqref{eq:27} holds.



    \par (Proof of existence of $\int_a^c f \; \mathrm{d}\alpha$) Assume $\int_a^b f \; \mathrm{d}\alpha$ and $\int_c^b f \; \mathrm{d}\alpha$ exist. Then 
    \begin{align*}
        \abs{S(P, f, \alpha) - \int_a^b f \; \mathrm{d}\alpha} < \varepsilon / 2
        \quad \forall P \supset P_\varepsilon
    \end{align*}
    for some $P_\varepsilon$ on $[a, b]$. And
    \begin{align*}
        \abs{S(P^{\prime\prime}, f, \alpha) - \int_c^b f \; \mathrm{d}\alpha} < \varepsilon / 2
        \quad \forall P^{\prime\prime} \supset P_\varepsilon^{\prime\prime}
    \end{align*}
    for some $P_\varepsilon^{\prime\prime}$ on $[c, b]$.
    Let
    \begin{align*}
        P_\varepsilon^\prime = (P_\varepsilon \cup P_\varepsilon^{\prime\prime}) \cap [a, c]
    \end{align*}
    Let
    \begin{align*}
        P^\prime &\supset P_\varepsilon^\prime &
        P^{\prime\prime} &\supset (P_\varepsilon \cup P_\varepsilon^{\prime\prime}) \cap [c, b] &
        P &= P^\prime \cup P^{\prime\prime}
    \end{align*} 
    Observe that 
    \begin{align*}
        P^{\prime\prime} &\supset P_\varepsilon^{\prime\prime} &
        P &= P^\prime \cup P^{\prime\prime}
        \supset (P_\varepsilon \cap [a, c]) \cup (P_\varepsilon \cap [c, b])
        = P_\varepsilon
    \end{align*}
    It then follows from \eqref{eq:26} that
    \begin{align*}
        &\abs{
            S(P^\prime, f, \alpha)
            - \int_a^b f \; \mathrm{d}\alpha
            + \int_c^b f \; \mathrm{d}\alpha 
        } \\ 
        &\leq \abs{
            S(P, f, \alpha)
            - \int_a^b f \; \mathrm{d}\alpha
        } + \abs{
            S(P^{\prime\prime}, f, \alpha)
            - \int_c^b f \; \mathrm{d}\alpha
        } \\ 
        &< \varepsilon / 2 + \varepsilon / 2 \\ 
        &= \varepsilon
    \end{align*} 
    Therefore, $f \in \mathfrak{R}(\alpha)$ on $[a, c]$ and \eqref{eq:27} holds.

\end{proof}

%------------------------------

\section{Integration by Parts}

%------------------------------

\begin{theorem}[Integration by Parts] \label{thm:20}
    If $f \in \mathfrak{R}(\alpha)$ on $[a, b]$, then $\alpha \in \mathfrak{R}(f)$ on $[a, b]$, and 
    \begin{align*}
        \int_a^b f \; \mathrm{d}\alpha
        + \int_a^b \alpha \; \mathrm{d}f
        = f(b)\alpha(b) - f(a)\alpha(a)
    \end{align*}
\end{theorem}

\begin{remark}
    This can be treated as the \textit{reciprocity law} for integrals.
\end{remark}

\begin{proof}
    Given $\varepsilon > 0$, since $f \in \mathfrak{R}(\alpha)$ on $[a, b]$, there exists a partition $P_\varepsilon$ such that 
    \begin{align}
        \abs{S(P, f, \alpha) - \int_a^b f \; \mathrm{d}\alpha} < \varepsilon
        \quad \forall P \supset P_\varepsilon
        \label{eq:28}
    \end{align}
    Let $P = \left\{x_0, x_1, \ldots, x_n\right\} \supset P_\varepsilon$ be any refinement of $P_\varepsilon$. The Riemann-Stieltjes sum of $\alpha$ with respect to $f$ is 
    \begin{align}
        S(P, \alpha, f)
        = \sum_{k=1}^n \alpha(t_k) (f(x_k) - f(x_{k-1}))
        = \sum_{k=1}^n \alpha(t_k) f(x_k)
        - \sum_{k=1}^n \alpha(t_k) f(x_{k-1})
        \label{eq:29}
    \end{align}
    Let 
    \begin{align}
        P^\ast = P \cup \set{t_k}{1 \leq k \leq n}
        \label{eq:30}
    \end{align}
    Denote by $A$ the value 
    \begin{align*}
        A = f(b)\alpha(b) - f(a)\alpha(a)
    \end{align*}
    Note that $A$ can be written as 
    \begin{align}
        A = \sum_{k=1}^n f(x_k) \alpha(x_k)
        - \sum_{k=1}^n f(x_{k-1}) \alpha(x_{k-1})
        \label{eq:31}
    \end{align}
    Subtracting \eqref{eq:29} from \eqref{eq:31}, we obtain
    \begin{align*}
        A - S(P, \alpha, f)
        = \sum_{k=1}^n f(x_k) (\alpha(x_k) - \alpha(t_k))
        + \sum_{k=1}^n f(x_{k-1}) (\alpha(t_k) - \alpha(x_{k-1}))
    \end{align*}
    By recalling the construction of $P^\ast$ in \eqref{eq:30}, we observe that the right-hand side of the above equation is precisely the Riemann-Stieltjes sum $S(P^\ast, f, \alpha)$. That is, 
    \begin{align*}
        A - S(P, \alpha, f) = S(P^\ast, f, \alpha)
    \end{align*}
    Since $P^\ast \supset P \supset P_\varepsilon$, it follows from \eqref{eq:28} that 
    \begin{align*}
        \abs{A - S(P, \alpha, f) - \int_a^b f \; \mathrm{d}\alpha} 
        = \abs{S(P^\ast, f, \alpha) - \int_a^b f \; \mathrm{d}\alpha}
        < \varepsilon
    \end{align*}
    Recall $A = f(b)\alpha(b) - f(a)\alpha(a)$, we have 
    \begin{align*}
        \abs{S(P, \alpha, f) + \int_a^b f \; \mathrm{d}\alpha - f(b)\alpha(b) + f(a)\alpha(a)} < \varepsilon
        \quad \forall P \supset P_\varepsilon
    \end{align*}
    This implies that $\alpha \in \mathfrak{R}(f)$ on $[a, b]$, and 
    \begin{align*}
        \int_a^b f \;\mathrm{d}\alpha
        = -\int_a^b f \;\mathrm{d}\alpha 
        + f(b)\alpha(b) - f(a)\alpha(a)
    \end{align*}
\end{proof}

%------------------------------

\section{Change of Variables}

%------------------------------

\begin{theorem} \label{thm:21}
    Suppose $f \in \mathfrak{R}(\alpha)$ on $[a, b]$, and $\phi$ is a \textbf{strictly monotonic continuous} function on a closed interval $I$ with endpoints $c$ and $d$. ($I$ is either $[c, d]$ or $[d, c]$.) Assume
    \begin{align*}
        a &= \phi(c) & b &= \phi(d)
    \end{align*}
    Define two composite functions:
    \begin{align*}
        h &= f(\phi(x)) & 
        \beta &= \alpha(\phi(x))
    \end{align*}
    Then $h \in \mathfrak{R}(\beta)$ on $I$, and $\int_a^b f \; \mathrm{d}\alpha = \int_c^d h \; \mathrm{d}\beta$, i.e., 
    \begin{align*}
        \int_a^b f(x) \; \mathrm{d}\alpha(x) = \int_{\phi^{-1}(a)}^{\phi^{-1}(b)} f(\phi(x)) \; \mathrm{d}\alpha(\phi(x))
    \end{align*}
\end{theorem}

\begin{remark}
    The reason why we assume that $\phi$ is strictly monotonic and continuous is to ensure that it is a bijective function. It is equivalent to assuming $\phi$ is strictly monotonic and injective.
\end{remark}

\par The main idea of this proof is exploiting the one-to-one relation between the partitions on $[a, b]$ and $[c, d]$. 

\begin{proof}
    (One-To-One Relation of Partitions) Without loss of generality, we may assume that $g$ is strictly \textit{increasing} and continuous. Then $I = [c, d]$. From the conditions of $g$, we can immediately conclude that it has a bijective inverse function
    \begin{align*}
        \phi^{-1}: [a, b] \to [c, d]
    \end{align*}
    For any partition $P^\prime = \left\{x_0, \ldots, x_n\right\}$ on $[a, b]$, we can associate it with a partition $P$ on $[c, d]$, which is given by 
    \begin{align*}
        P := \phi^{-1}(P^\prime) 
        := \left\{\phi^{-1}(x_0), \ldots, \phi^{-1}(x_n)\right\}
    \end{align*}
    On the other hand, for any partition $P = \left\{y_0, \ldots, y_n\right\}$ on $[c, d]$, we can define a partition $P^\prime$ on $[a, b]$ by 
    \begin{align*}
        P^\prime := \phi(P) := \left\{\phi(y_0), \ldots, \phi(y_n)\right\}
    \end{align*}

    \par (Existence of the Integral) Given $\varepsilon > 0$, since $f \in \mathfrak{R}(\alpha)$ on $[a, b]$, there exists a partition $P^\prime_\varepsilon$ on $[a, b]$ such that 
    \begin{align}
        \abs{S(P^\prime, f, \alpha) - \int_a^b f \; \mathrm{d}\alpha} < \varepsilon
        \quad \forall P^\prime \supset P^\prime_\varepsilon
        \label{eq:32}
    \end{align}
    Let partition $P_\varepsilon$ on $[c, d]$ be given by $P_\varepsilon = \phi^{-1}(P^\prime_\varepsilon)$. For any refinement $P \supset P_\varepsilon$, we have 
    \begin{align*}
        S(P, h, \beta)
        = \sum h(s_i) \Delta\beta_i
        = \sum h(s_i) (\beta(s_i) - \beta(s_{i-1}))
    \end{align*}
    For each point $s_i$, we can map it to $[a, b]$ by $t_i = \phi(s_i)$. It then follows that 
    \begin{align*}
        S(P, h, \beta)
        &= \sum h(\phi^{-1}(t_i)) (\beta(\phi^{-1}(t_i)) - \beta(\phi^{-1}(t_{i-1}))) \\ 
        &= \sum f(t_i) (\alpha(t_i) - \alpha(t_{i-1})) \\ 
        &= S(P^\prime, f, \alpha)
    \end{align*}
    where $P^\prime = \phi(P)$. In summary,
    \begin{align}
        S(P, h, \beta) = S(P^\prime, \phi, \alpha)
        \label{eq:33}
    \end{align}
    What is left to show is that $P^\prime \supset P^\prime_\varepsilon$. For any point $x \in P^\prime_\varepsilon$, we have $\phi^{-1}(x) \in P_\varepsilon$ since $P_\varepsilon = \phi^{-1}(P_\varepsilon)$. Recall that $P \supset P_\varepsilon$. Thus, $\phi^{-1}(x) \in P$. And because $P^\prime = \phi(P)$, we have $x = \phi(\phi^{-1}(x)) \in P^\prime$. Therefore, indeed $P^\prime \supset P^\prime_\varepsilon$. It then follows from \eqref{eq:32} and \eqref{eq:33} that 
    \begin{align*}
        \abs{S(P, h, \beta) - \int_a^b f \; \mathrm{d}\alpha} < \varepsilon
        \quad \forall P \supset P_\varepsilon
    \end{align*}
    This completes the proof.
\end{proof}

%------------------------------

\section{Monotonically Increasing Integrators}

\par It is much easier to study and prove some properties of the Riemann-Stieltjes integrals if we require that $\alpha$ is monotonically increasing. 

\par Thanks to Theorem~\ref{thm:22}, which states that every function of bounded variation can express as a difference of two increasing functions, almost all the properties introduced in this section can be extended with ease to integrators of bounded variation. 

%==============================
% print index page
\printindex

%==============================
\end{document}